{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning Workshop\n",
    "---\n",
    "Welcome to the HackIllinois 2020 Intro to ML in Python technical workshop! In this notebook, we will be taking a deep dive into the hello world of machine learning topics - regression. The contents of this notebook will be organized as follows:\n",
    "\n",
    "### Table of Contents:\n",
    "1. Overview of Machine Learning\n",
    "2. The Data\n",
    "3. Linear Regression\n",
    "4. Regularization\n",
    "\n",
    "### Notation\n",
    "- scalar variables will be lowercase e.g. x, y\n",
    "- vectors will be bold lowercase e.g. **u, v**\n",
    "- matrices will be bold uppercase e.g. **A, B**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview of Machine Learning\n",
    "\n",
    "On a high level, the goal of machine learning is to leverage models or algorithms that can learn from data, so that we can reason about the data generating processes in our world. The applications of machine learning are vast because so many fields and industries collect and analyze data. Some of the domains that utilize machine learning include:\n",
    "- business\n",
    "- medicine\n",
    "- engineering\n",
    "- physics\n",
    "- transportation\n",
    "- real estate\n",
    "- finance\n",
    "- military & defense\n",
    "- commerce\n",
    "\n",
    "And here are some cool machine learning applications:\n",
    "***\n",
    "[Object Tracking](https://nanonets.com/blog/object-tracking-deepsort/)\n",
    "![](https://nanonets.com/blog/content/images/2019/07/messi_football_track.gif)\n",
    "***\n",
    "[Robotics & Control](https://bair.berkeley.edu/blog/2020/06/25/D4RL/)\n",
    "![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592520372323_kitchen-mixed.gif)\n",
    "***\n",
    "[Image Semantic Segmentation](https://ai.googleblog.com/2020/07/improving-holistic-scene-understanding.html)\n",
    "![](https://1.bp.blogspot.com/-oSVo5zrtc_Y/Xxc5yMjPndI/AAAAAAAAGP8/yLbgy8VsfeoSY_5TTgJSjupW5QGRtjjewCLcBGAsYHQ/s1600/image2.gif)\n",
    "***\n",
    "[Machine Translation](https://google.github.io/seq2seq/)\n",
    "![](https://3.bp.blogspot.com/-3Pbj_dvt0Vo/V-qe-Nl6P5I/AAAAAAAABQc/z0_6WtVWtvARtMk0i9_AtLeyyGyV6AI4wCLcB/s1600/nmt-model-fast.gif)\n",
    "***\n",
    "[Protein Folding](https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery)\n",
    "![](https://i1.wp.com/syncedreview.com/wp-content/uploads/2018/12/Origami-CASP-181127-r01_fig3-results-anim-crop.gif?resize=800%2C267&ssl=1)\n",
    "***\n",
    "[Weather Prediction](https://ai.googleblog.com/2020/03/a-neural-weather-model-for-eight-hour.html)\n",
    "![](https://1.bp.blogspot.com/-8un0Nu92sxg/XnpmRAwVH7I/AAAAAAAAFj0/44DiKjDlTfc8JxaaSKZyjcP5W8UMDzdiwCLcBGAsYHQ/s1600/image1.gif)\n",
    "***\n",
    "### Types of Machine Learning\n",
    "Generally machine learning algorithms fall into three categories\n",
    "1. Supervised Learning: Learning from labeled data pairs. \n",
    "\n",
    "    - an image of cat and a label 'cat'\n",
    "    - a speech waveform and an associated english sentence\n",
    "    - an Amazon product review and the 0-5 star rating. \n",
    "    \n",
    "    \n",
    "2. Unsupervised Learning - learning from unlabeled data \n",
    "\n",
    "    - images scraped from Google Images\n",
    "    - all of the [text](https://openai.com/blog/openai-api/) on the internet\n",
    "    - sensory experiences from the world around you! \n",
    "    \n",
    "    \n",
    "3. Semi-supervised Learning - learning from a mix of labeled and unlabeled data\n",
    "\n",
    "    - a large unlabeled dataset of images & a small labeled dataset of dogs\n",
    "    \n",
    "It is important to note that the line of distinction between supervised and unsupervised learning is blurry. Recently, there has been a rush of new research in [self-supervised learning](https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html) which is essentially creating labels from unlabeled datasets in clever ways in order to learn representations of data.\n",
    "\n",
    "Today, we will focus exclusively on regression, a supervised learning task... **we're only barely scratching the surface!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get started by importing the Python packages we need\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# import the visualization tools\n",
    "from visualization import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Data\n",
    "\n",
    "Working with data often takes up a **significant** amount of time when doing machine learning in practice, or as the academics like to say 'in the wild'. Many things about real world datasets can be nuisances, such as:\n",
    "- noise corrupted data\n",
    "- missing values\n",
    "- incorrect labelings\n",
    "- duplicate examples\n",
    "- mixed data types (numerical, categorical)\n",
    "- large magnitude differences\n",
    "- too much data\n",
    "- too little data/labeled data\n",
    "- correlated (redundant) data features\n",
    "\n",
    "It's important to be diligent when working with data because incorrect data will render your model useless! \n",
    "***\n",
    "#### The Dataset\n",
    "The dataset we'll use today is from [Machine Learning in R](https://www.kaggle.com/mirichoi0218/insurance/data), and it will contain 1338 medical records of patient attributes and corresponding health insurance costs. The attributes included in the dataset are:\n",
    "- age: age of primary beneficiary (integer)\n",
    "\n",
    "- sex: insurance contractor gender, female, male (categorical)\n",
    "\n",
    "- bmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height (real)\n",
    "\n",
    "- children: Number of children covered by health insurance / Number of dependents (integer)\n",
    "\n",
    "- smoker: Smoking (categorical)\n",
    "\n",
    "- region: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest (categorical)\n",
    "\n",
    "- charges: Individual medical costs billed by health insurance (real)\n",
    "\n",
    "\n",
    "We will try to predict the health insurance costs based off of the patient attributes.\n",
    "***\n",
    "Pandas is an extremely useful data analysis library that features an object called the DataFrame that allow the data scientist to manipulate data efficiently. Let's use the Pandas library to read in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dataset from the data directory\n",
    "df = pd.read_csv(\"data/insurance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "___\n",
    "\n",
    "### Data Basics\n",
    "1. The data that are inputs to your learning algorithm are sometimes called **covariates**, **predictors**, or **features**. For this tutorial we will now refer to the input data as **features**. \n",
    "\n",
    "\n",
    "2. Data can be either **categorical** (e.g. country, sex, school, etc), or **numeric** (e.g. temperature, cost, height, etc). In machine learning, categorical data often need to be preprocessed into numeric forms so that our numeric models can process them. There are many ways to do this, for example, [word embeddings](https://en.wikipedia.org/wiki/Word_embedding) for text, or [one-hot](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) encoding (sometimes also called 1-of-K or dummy encoding). \n",
    " \n",
    " \n",
    "3. If you are operating in a supervised learning setting, the outputs you are trying to predict are sometimes called **variates** or **labels**, we'll stick to **labels** for now.\n",
    "\n",
    "In the context of our dataset, every column in the DataFrame are considered features, except for the **charges** column which will act as our labels. We'll also preprocess our categorical features with one-hot encodings.\n",
    "\n",
    "### Exploratory Data Analysis (EDA)\n",
    "**Exploratory data analysis** is the act of probing and analyzing your dataset to gain a better understanding of the dataset structure and/or to inform manipulations you will apply to it. The EDA step in tackling a machine learning problem in the wild is essential for preparing the data to be input into the model.\n",
    "\n",
    "We will first do a **bivariate analysis**, a fancy term for saying \"look at the correlation matrix\". We'll also visualize some plots for the bivariate pairs. For the categorical features, we'll visualize box and whisker plots, and scatter plots for the numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_corr_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_bivariates(df, \"charges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "[Feature selection](https://en.wikipedia.org/wiki/Feature_selection) is the process of selecting the data that will have the most 'impact' in our predictive model. By dropping columns that aren't important to us we can significantly reduce our model complexity and speed up the optimization process of the learning algorithm.\n",
    "\n",
    "- From the box and whisker plots, we can clearly see that smokers are much more likely to be charged more based off health insurance, while sex and region have little correlation on the medical costs. We'll drop these columns.\n",
    "\n",
    "\n",
    "- From the scatter plots, we can see roughly linear trends in the data for all the features so we'll keep all of these.\n",
    "\n",
    "\n",
    "- It is important to note that if you spot a high feature correlation (not on the diagonal) of the correlation matrix, it will be useful to drop those features becuase highly correlated features provide redundant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update our dataframe by dropping the columns we don't care about\n",
    "df_dropped = df.drop(columns=['sex', 'region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encodings\n",
    "We still have one categorical feature remaining, the **smoker** feature. How do we feed this data into our model and do matrix algebra with it? We need to convert it into a numerical feature so we'll rely on one-hot encoding.\n",
    "\n",
    "One hot encoding works by mapping each category to a unique positive integer. If you wanted to one-encode the set [\"dog\", \"cat\", \"fish\"], you could map it to [0, 1, 2]. The ordering is arbitrary, but it needs to stay consistent once you set it or else you could inadvertently mistake your model's output of 'dog' for 'cat'.\n",
    "\n",
    "Luckily for our case, **smoker** is a binary categorical variable so it should be easy to map it to {0, 1}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df_dropped.replace({'yes': 1, 'no': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "Imagine you have a dataset that include timer measurements from two different timers measuring two different physical processes (i.e. the timer measurements have little to no correlation). Now if one timer measures in nanoseconds (10^-9sec) and the other in seconds (10^0sec), and we want to feed this data into our machine learning algorithm then the data measured in nanoseconds will have very little learning signal and the seconds data will have most of the effect.\n",
    "\n",
    "This scenario highlights the issue of differing magnitudes in datasets. To alleviate this nuisance, we rely on [feature scaling](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e). There are many ways to scale your data, but perhaps the most popular way is standardization which simply involves subtracting the column mean and dividing by the column standard deviation:\n",
    "\n",
    "$$\\bar{x} = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "This scaling operation causes the data to have zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the column means and stddevs\n",
    "df_scaled = df_numeric[['bmi', 'age']]\n",
    "means = df_scaled.mean()\n",
    "stddevs = df_scaled.std()\n",
    "\n",
    "# transform the columns\n",
    "df_scaled -= means\n",
    "df_scaled /= stddevs\n",
    "\n",
    "# add the other columns\n",
    "other_cols = df_numeric.drop(columns=['age', 'bmi'])\n",
    "df_prepro = pd.concat([df_scaled, other_cols], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepro.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "Now that we have our data all nice and preprocessed we can just pick a learning algorithm and learn the whole dataset right? \n",
    "\n",
    "No, because there's no guarantee that the model we fit to our dataset actually captures the true data distribution. This is why we usually split our data into separate sets - one for training, and one for testing. This way we know if our model is actually a good fit to of the data, assuming the data are a good representation of the actual data distribution.\n",
    "\n",
    "It is also common to create a third split called the validation set, which helps to choose hyperparameters, but we'll won't worry about this for now. If you're interested, you can read up on [k-fold cross validation](https://machinelearningmastery.com/k-fold-cross-validation/).\n",
    "\n",
    "We'll take 80% of the data as training and 20% as testing, then we'll separate the features and labels and get to the fun part!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataframe into train and test (shuffle the data first)\n",
    "df_prepro = df_prepro.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# you'll see why we do this in a bit\n",
    "df_prepro.insert(0, 'aux', value=1)\n",
    "\n",
    "split = int(0.8 * len(df_prepro))\n",
    "train = df_prepro.iloc[:split]\n",
    "test = df_prepro.iloc[split:]\n",
    "\n",
    "# separate the features and labels\n",
    "columns = df_prepro.columns\n",
    "column_mask = np.array([0, 0, 0, 0, 0, 1], dtype=bool)\n",
    "\n",
    "train_x = train[columns[~column_mask]]\n",
    "train_y = train[columns[column_mask]]\n",
    "\n",
    "test_x = test[columns[~column_mask]]\n",
    "test_y = test[columns[column_mask]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_side_by_side([train_x, train_y, test_x, test_y],\n",
    "                     [\"train_x\", \"train_y\", \"test_x\", \"test_y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Linear Regression\n",
    "### The Linear Model\n",
    "Here's where things get fun. In this section, we'll take a deep dive into a very simple yet powerful model - the linear model. It turns out many situations permit a linear model of a lot of the natural processes of the world. After all, if you zoom in far enough on a curve you eventually see a line! Recall the equation for the line:\n",
    "\n",
    "$$y = mx + b$$\n",
    "\n",
    "If we want to extend this to multiple dimensions, just like how our dataset consists of 4-D [bmi, age, children, smoker] vectors, we'll utilize matrix notation:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= \\theta_0 + \\sum_{i=1}^{d} x_i\\theta_i\\\\ \n",
    "&= \\begin{bmatrix}1 & x_1 & \\ldots & x_{d-1} & x_d\\end{bmatrix}^\\top \\begin{bmatrix}\\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_{d-1} \\\\ \\theta_d\\end{bmatrix} \\\\\n",
    "&= \\mathbf{x}^\\top \\boldsymbol{\\theta}\n",
    "\\label{eq:pred} \\tag{1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $d$ is the dimensionality of the features. Notice how the $\\mathbf{x}$ vector was augmented with a 1 in the first entry which has the same effect as adding a bias when taking the dot product.\n",
    "\n",
    "Let's define a few things:\n",
    "$$\n",
    "\\textbf{X}=\n",
    "\\begin{bmatrix}\n",
    "\\rule[.5ex]{3.5em}{0.4pt} & \\mathbf{x}_1^\\top & \\rule[.5ex]{3.5em}{0.4pt} \\\\\n",
    "\\rule[.5ex]{3.5em}{0.4pt} & \\mathbf{x}_2^\\top & \\rule[.5ex]{3.5em}{0.4pt} \\\\\n",
    "&\\vdots& \\\\\n",
    "\\rule[.5ex]{3.5em}{0.4pt} & \\mathbf{x}_{N-1}^\\top & \\rule[.5ex]{3.5em}{0.4pt} \\\\\n",
    "\\rule[.5ex]{3.5em}{0.4pt} & \\mathbf{x}_N^\\top & \\rule[.5ex]{3.5em}{0.4pt} \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{N\\times(d+1)}\n",
    "$$\n",
    "- The design matrix. Contains all the data vectors stacked horizontally.\n",
    "\n",
    "$$\n",
    "\\pmb{\\theta} = \\begin{bmatrix}\n",
    "\\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_{d-1} \\\\ \\theta_d\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{(d+1)\\times 1}\n",
    "$$\n",
    "- The parameter vector. This is essentially encodes the entire model, we want to find this.\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = \\begin{bmatrix}\n",
    "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{N-1} \\\\ y_N\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{N\\times 1}\n",
    "$$\n",
    "- The prediction vector. Contains the model predictions for every point in the dataset. Similarly, $\\mathbf{y}$ is the label vector.\n",
    "\n",
    "Matching the matrix dimensions, we want (Nx1) outputs from our ((d+1)xN) design matrix and ((d+1)x1) parameter vector. So the linear model for the entire dataset is:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{y}} = \\mathbf{X}\\pmb{\\theta}\n",
    "\\label{eq:lin} \\tag{2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "### The Loss Function\n",
    "Given a parameter vector $\\pmb{\\theta}$, we want to know how good it is at predicting the data. We can give it a score using an **objective** or **cost** or **loss** function. A common loss function used for linear regression is the sum-of-squared-errors loss function:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\pmb{\\theta}| \\mathbf{X}, \\mathbf{y}) &= \\frac{1}{2}\\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2 \\\\\n",
    "&= \\frac{1}{2}\\sum_{i=1}^{N}(y_i - \\mathbf{x}_i^\\top \\pmb{\\theta})^2 \\\\\n",
    "&= \\frac{1}{2}(\\mathbf{y} - \\mathbf{X}\\pmb{\\theta})^\\top(\\mathbf{y} - \\mathbf{X}\\pmb{\\theta})\\\\\n",
    "&= \\frac{1}{2}\\left\\|\\mathbf{y} - \\mathbf{X}\\pmb{\\theta}\\right\\|_{2}^{2}\n",
    "\\label{eq:loss} \\tag{3}\n",
    "\\end{align}\\\\\n",
    "$$\n",
    "If this choice of loss function seems somewhat arbitrary, it's because the motivation is out of scope of this tutorial. If you're interested in learning why the sum-of-squared-errors loss is a natural choice for the loss function you can read [this](https://www.cs.princeton.edu/courses/archive/fall18/cos324/files/mle-regression.pdf). \n",
    "\n",
    "A nice interpretation of sum-of-squared-errors loss involves placing one end of a spring at every data point and the other at the point on the line directly above it. Assuming the spring are all equal strength, the line is free to move, the data points are fixed, and there is no gravity, the line would eventually oscillate towards an equilibrium - the best fit line! Here's a visualization:\n",
    "![](https://www.jmp.com/en_us/statistics-knowledge-portal/what-is-regression/the-method-of-least-squares/_jcr_content/par/styledcontainer_2069/par/lightbox_375d_copy/lightboxImage.img.png/1595273803818.png)\n",
    "\n",
    "Let's implement this loss function and see how well we do for a randomly intialized parameter vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sse(labels, predictions):\n",
    "    return 0.5 * np.linalg.norm(labels - predictions[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_random = np.random.randn(train_x.shape[-1])\n",
    "error = sse(test_y, test_x @ theta_random)\n",
    "print(f\"Random params average sum-of-squared-errors loss: {error / len(test_y):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "The question that remains is: how do we get the best fit line?\n",
    "\n",
    "Notice that the loss function is measure of how good our current parameter estimate $\\pmb{\\theta}$ is given the data. Lower is better. So if we have the parameters that describe the best fit line, then the loss should be pretty low - in fact it should be at the minimum possible value! \n",
    "\n",
    "To find the optimal $\\pmb{\\theta}$, we just have to differentiate w.r.t. $\\pmb{\\theta}$ and set the expression to zero. By doing this, we are **guaranteed** to find **the** best fit line since the loss function is quadratic - there is only *one* minimum, hence the best.\n",
    "\n",
    "Before we dive into a beautiful derivation of the solution to the least squares problem, we have to define some matrix derivatives. Convince yourself that these are true.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathbf{x}^\\top\\mathbf{A}}{\\partial\\mathbf{x}} = \\mathbf{A} &&\n",
    "\\frac{\\partial \\mathbf{x}^\\top\\mathbf{A}\\mathbf{x}}{\\partial\\mathbf{x}} = 2\\mathbf{A}\\mathbf{x}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now we can dive in. Start with the sum-of-squared-errors loss function and its derivative w.r.t. $\\pmb{\\theta}$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\pmb{\\theta}}\\mathcal{L}(\\pmb{\\theta}| \\mathbf{X}, \\mathbf{y}) &= \\frac{\\partial}{\\partial \\pmb{\\theta}} [\\frac{1}{2}\\left\\|\\mathbf{y} - \\mathbf{X}\\pmb{\\theta}\\right\\|_{2}^{2}]\\\\\n",
    "&= \\frac{\\partial}{\\partial \\pmb{\\theta}}[\\frac{1}{2}(\\mathbf{y} - \\mathbf{X}\\pmb{\\theta})^\\top(\\mathbf{y} - \\mathbf{X}\\pmb{\\theta})]\\\\\n",
    "&= \\frac{1}{2}\\frac{\\partial}{\\partial \\pmb{\\theta}}[\\mathbf{y}^\\top\\mathbf{y} - \\mathbf{y}^\\top\\mathbf{X}\\pmb{\\theta} - \\pmb{\\theta}^\\top\\mathbf{X}^\\top\\mathbf{y} + \\pmb{\\theta}^\\top\\mathbf{X}^\\top\\mathbf{X}\\pmb{\\theta} ]\\\\\n",
    "&= \\frac{1}{2}\\frac{\\partial}{\\partial \\pmb{\\theta}}[\\mathbf{y}^\\top\\mathbf{y} - 2\\pmb{\\theta}^\\top\\mathbf{X}^\\top\\mathbf{y} + \\pmb{\\theta}^\\top\\mathbf{X}^\\top\\mathbf{X}\\pmb{\\theta} ]\\\\\n",
    "&= \\frac{1}{2}[-2\\mathbf{X}^\\top\\mathbf{y} + 2\\mathbf{X}^\\top\\mathbf{X}\\pmb{\\theta}]\\\\\n",
    "&= -\\mathbf{X}^\\top\\mathbf{y} + \\mathbf{X}^\\top\\mathbf{X}\\pmb{\\theta}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where moving from the $3^{rd}$ to $4^{th}$ line used the fact that $\\pmb{\\theta}^\\top\\mathbf{X}^\\top\\mathbf{y} = \\mathbf{y}^\\top\\mathbf{X}\\pmb{\\theta}$ since the LHS is a (1x(d+1))((d+1)xN)(Nx1) = (1x1) matrix product that returns a scalar and the transpose of a scalar is itself so the equality follows. And the $4^{th}$ to the $5^{th}$ utilized our matrix derivate rules. Now that we have the derivative w.r.t. $\\pmb{\\theta}$, we can just set it to zero to get the optimal parameters $\\pmb{\\theta}^*$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-\\mathbf{X}^\\top\\mathbf{y} + \\mathbf{X}^\\top\\mathbf{X}\\pmb{\\theta}^* &= 0\\\\\n",
    "\\mathbf{X}^\\top\\mathbf{X}\\pmb{\\theta}^* &= \\mathbf{X}^\\top\\mathbf{y}\\\\\n",
    "(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{X}\\pmb{\\theta}^* &= (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y} \\\\\n",
    "\\pmb{\\theta}^* &= (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}\n",
    "\\label{eq:sol} \\tag{4}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The matrix $(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top$ is also known as the [Moore-Penrose Pseudoinverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) which acts as a generalization of the matrix inverse and can be applied to non-square matrices. That's it! We can now implement this matrix algebra using NumPy's '@' operator which denotes matrix multiplication and the '.T' attribute which returns the transpose of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.linalg.inv(train_x.T @ train_x) @ train_x.T @ train_y\n",
    "theta = np.array(theta.values).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Optimal parameters: \\n{theta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run predictions on the test set using our learn parameters\n",
    "predictions = test_x @ theta\n",
    "train_predictions = train_x @ theta\n",
    "test_error = sse(test_y, predictions)\n",
    "train_error = sse(train_y, train_predictions)\n",
    "print(f\"Optimal params average sum-of-squared-errors train loss: {train_error / len(train_y):.2f}\")\n",
    "print(f\"Optimal params average sum-of-squared-errors test loss: {test_error / len(test_y):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Equally as important as the data preprocessing and the learning algorithm, is the analysis after the fact. In fact, there is a [whole subfield](https://christophm.github.io/interpretable-ml-book/) dedicated to interpreting trained machine learning models. We will visualize the numeric bivariates once again, but this time with the lines learned from the model, and well also try to interpret the results of the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_bivariates(df_prepro.drop(columns=['aux']), \"charges\", 4, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the best fit line through the data strikes through the densest regions, which is what we would expect as a solution - the more data points there are, the more they contribute towards the error. So the model is okay with leaving some point clouds untouched.\n",
    "\n",
    "Additionally, notice how the last parameter in the optimal parameter vector is the largest in magnitude. This particular parameter corresponds to the **smoker** feature, which we expected at the beginning to have a high impact, and now we can see that impact directly in the parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Regularization\n",
    "Is our linear model the *best* linear model for this data? What if those regions of diffuse point clouds were corrupted data or outliers? What if our parameter values are too high? What would a lower norm solution look like? Essentially, our model may be **overfitting** which means the parameters are too finetuned to the training set and thus provides over-confident, poor predictions on the test set.\n",
    "\n",
    "We want to **regularize** our model which seeks to prevent overfitting by penalizing model complexity. How do we measure model complexity? Like many topics in machine learning, there are many ways to do so, but generally you would measure the model complexity as a function of the parameters. To penalize complexity, we can just add a term in the loss function:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\pmb{\\theta}| \\mathbf{X}, \\mathbf{y})= \\frac{1}{2}\\left\\|\\mathbf{y} - \\mathbf{X}\\pmb{\\theta}\\right\\|_{2}^{2} + \\lambda R(\\pmb{\\theta})\n",
    "\\label{eq:rloss} \\tag{5}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The constant $\\lambda$ controls the strength of the regularization effect. For this tutorial we will look at $R(\\pmb{\\theta}) = \\frac{1}{2}\\left\\|\\pmb{\\theta}\\right\\|_{2}^{2}$. This complexity measure penalizes parameters with large magnitudes and is called **L2 regularization** or **Tikhonov regularization**. The good thing about this new loss is that it also has a global minimum!\n",
    "\n",
    "Using the same procedure from before of differentiating with respect to $\\pmb{\\theta}$ and setting the derivative equal to 0, the solution for the optimal parameter vector is:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\pmb{\\theta}^* = (\\mathbf{X}^\\top\\mathbf{X} + \\lambda \\mathbf{I})^{-1}\\mathbf{X}^\\top\\mathbf{y}\n",
    "\\label{eq:ridge} \\tag{6}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{I}$ is the identity matrix. The proof for deriving this equation is left as an exercise for the reader. This solution is called **ridge regression**, which gets it's name from how the optimization is constrained along a contour of the regularization surface. If you're interested, you can go through these [slides](https://www.stat.cmu.edu/~ryantibs/datamining/lectures/16-modr1.pdf). If $R(\\pmb{\\theta}) = \\left\\|\\pmb{\\theta}\\right\\|_{1}$, then it is called [lasso regression](http://www.few.vu.nl/~wvanwie/Courses/HighdimensionalDataAnalysis/WNvanWieringen_HDDA_Lecture56_LassoRegression_20182019.pdf).\n",
    "\n",
    "__Fun fact__: ridge regression came from the 70's, when computer scientist noticed that computing $(\\mathbf{X}^\\top\\mathbf{X})^{-1}$ was problematic because the matrix was often [poorly conditioned](http://www.cse.iitd.ernet.in/~dheerajb/CS210_lect07.pdf), so they added a small term ($\\lambda \\mathbf{I}$) to the diagonal for numerical stability. To their surprise, this hack seemed to give better predictions and spawned a rich branch of theoretical machine learning!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regress(x, y, l):\n",
    "    theta = np.linalg.inv(x.T @ x + l * np.eye(x.shape[-1])) @ x.T @ y\n",
    "    return np.array(theta.values).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the test sum-of-squared-error as a function of $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of lambdas to try\n",
    "lambdas = np.linspace(0, 10, 100)\n",
    "\n",
    "# store test results and best performing params\n",
    "test_errors = []\n",
    "best_theta = None\n",
    "best_error = float('inf')\n",
    "best_lambda = 0\n",
    "for l in lambdas:\n",
    "    optimal_params = ridge_regress(train_x, train_y, l)\n",
    "    ridge_prediction = test_x @ optimal_params\n",
    "    ridge_error = sse(test_y, ridge_prediction)\n",
    "    if ridge_error < best_error:\n",
    "        best_theta = optimal_params\n",
    "        best_error = ridge_error\n",
    "        best_lambda = l\n",
    "    test_errors.append(ridge_error)\n",
    "    \n",
    "# plot the results\n",
    "_ = (\n",
    "    plt.figure(figsize=(8, 6)),\n",
    "    plt.title(\"Effect of L2 regularization\"),\n",
    "    plt.xlabel(\"$\\lambda$\"),\n",
    "    plt.ylabel(\"Test error\"),\n",
    "    plt.plot(lambdas, test_errors)\n",
    ")\n",
    "print(f\"Best average test error: {best_error/len(test_y):.2f} | Best lambda: {best_lambda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out the best error doesn't occur at $\\lambda = 0$! If we compare, the new best parameter vector with the ordinary linear regression solution we can see that the magnitudes of the parameters are generally smaller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Previous theta: {theta}\")\n",
    "print(f\"New theta: {best_theta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the bivariates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_bivariates(df_prepro.drop(columns=['aux']), \"charges\", 4, best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much changed. What if we sacrificed test loss, and cranked up $\\lambda$? What about make lambda negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularized_theta = ridge_regress(train_x, train_y, l=-50)\n",
    "show_bivariates(df_prepro.drop(columns=['aux']), \"charges\", 4, regularized_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
